// This file is generated by build-grid.js. Do not edit by hand.
var GRID_DATA = [
  {
    "color": "black",
    "width": 8,
    "height": 1,
    "title": "Tony Edvalson",
    "contentInGrid": "false",
    "subtitle": "SWE, ML, Founder",
    "category": "Hi there!",
    "extraClass": "title",
    "themes": "true",
    "order": "000",
    "source": "tonyedv/000 intro.md",
    "markdown": "I love to tinker with side projects, like experimenting with ML systems and creative writing. And to relax I love to go for a run, or mess around in sandbox video games like Factorio and Minecraft."
  },
  {
    "color": "black",
    "width": 4,
    "code": "PRJ.098",
    "title": "Atlas",
    "subtitle": "LLM powered personal assistant",
    "height": 5,
    "chip": "atlas",
    "category": "LLMs",
    "date": "\"2025\"",
    "order": "001",
    "source": "tonyedv/001 Atlas.md",
    "markdown": "Atlas is an LLM powered personal assistant. It‚Äôs my take on a Siri-like system that I spent about a year building and iterating on. I had a few goals with this project, but the main ones were to push the boundaries on AI systems and figure out the best way to build AI products.\n\nAs part of this, I tried every AI app on the App Store to see what ideas were out there. I concluded that they all suck and everyone is just copying ChatGPT blindly with no imagination. I mean seriously, even the logos all look the same:\n\n![](assets/atlas-apps.png)\n\nSo I wanted to see what would happen if someone tried to build something more opinionated and break all the expectations that OpenAI has established for the industry.\n\n# But first, what does Atlas do?\n\nLots of stuff! I‚Äôve hooked Atlas into every API I can. This includes:\n\n- Email\n- Calendar\n- Push notifications\n- Health data (sleep, steps, activity, stress)\n- Todo lists (asana)\n- Lights (hue)\n- Read / write files\n- Timers\n- Weather\n- Location\n- Phone battery\n\n# The AI\n\nMy goal here was to make a ‚Äúunified‚Äù AI system. This is in contrast to pretty much every product out there, which is structured in these disconnected and stateless conversations. This makes sense for ChatGPT, but when you‚Äôre having a personal assistant who‚Äôs keeping track of everything, it‚Äôs the wrong model. I want a system like Jarvis, where it‚Äôs basically an independent thing with experiences and a continuous perspective.\n\nThis means there‚Äôs only one thread of messages that is never cleared or reset. And all work is done in that one thread. This also means that Atlas doesn‚Äôt have different prompt structures for different situations. So there‚Äôs only one interface to Atlas:\n\n```\nAtlas.ask(task, channel)\n```\n\nTask is a prompt saying what you want atlas to do. And channel splits conversations between internal and external thoughts, we‚Äôll get into that more later, but it‚Äôs a way to make sure an incoming email doesn‚Äôt interrupt Atlas‚Äôs analysis.\n\n## Why Unified AI?\n\nI think there‚Äôs a lot of non-obvious benefits to this philosophy.\n\nOne is that the AI gets all the info all the time, there‚Äôs no weird boundaries where one LLM is filtering data for another, and that output is reviewed by another system. All functionality and information is available everywhere.\n\nThis has lots of little benefits, like when I‚Äôm about to leave for the day, Atlas will give me a heads up if there‚Äôs rain, or my phone battery is low.\n\nOne time, as a test, I added a task in asana saying ‚Äúhi atlas! Can you turn all my lights blue?‚Äù and it did. I‚Äôm not sure that counts as a feature or a prompt injection, but it‚Äôs cool nonetheless.\n\n\n\n## Prompt\n\nThe prompt has a couple parts:\n\n**1. Personality**\nThis is the one part of the prompt that I didn‚Äôt write. It‚Äôs stored in a file that Atlas can edit and update as needed. The persona is pretty unconventional, but we‚Äôll take a closer look at it later on.\n\n**2. Holidays**\nThere‚Äôs also an optional ‚Äúholiday‚Äù prompt that is added on certain days encouraging Atlas to use certain emojis or nudge behavior. Like using eagles and flags on the 4th of July ü¶Öü¶Öü¶Ö\n\n**3. Task**\nAfter that is whatever task was passed into atlas.ask()\n\n**4. Features**\nThis section has various prompts that are injected by the features. There are ~20 different features that Atlas supports and each can specify a prompt to slot into the final prompt. This includes things that Atlas should be generally aware of, like my location, task list, calendar events, etc.\n\n**5. Channel**\nThis is the active conversation thread with summaries of old messages.\n\n## Feature System (ECS)\n\nEntity component systems are really nice for AI systems, and is how I plan to structure all my AI products going forward.\n\nThe core idea is to not have a giant messy file stitching together a huge prompt, a giant messy file specifying all the tools, a bunch of files with the implementations of those tools, and so on.\n\nInstead, you build a simple ‚ÄúAI Engine‚Äù that functions like a game engine, and you slot in individual features that hook into that engine.\n\nFor example, Atlas has an AsanaTasks feature. This feature contains a tool definition for creating new tasks, and code that generates text to inject in the guidance, as well as some timed events that poll for new data every few minutes. This means that everything is defined in one file and we don‚Äôt need any crazy prompt templating / compositing engine since everything is broken down into easy to use parts.\n\nAlso, coding agents love this format since it‚Äôs immediately obvious where a given feature lives, and everything is self contained, no long-range dependencies on other systems.\n\n## Channels\n\nChannels are used to store message history. They use a hierarchical compaction system to manage memory, which allows Atlas to have conversations without ever needing to reset the conversation.\n\nNew messages are stored in their raw form, but once there's enough to hit a token limit, the oldest half of them are ‚ÄúL1 compressed‚Äù. Which uses a LLM to keep all the key info, but tries to phrase it more succinctly. Then when there‚Äôs too many L1 messages, they get ‚ÄúL2 compressed‚Äù, which just keeps high level details. Then when there‚Äôs too many of those they get ‚ÄúL3 compressed‚Äù which removes almost everything except major events.\n\nThis system works very well, mostly because you have a lot of settings and compression prompts that can be tweaked to retain the information you want. The best I've seen it work is when Atlas referenced a throwaway joke I made from several weeks ago after it had been compressed a few times.\n\nLater I added (at Atlas‚Äôs request) a second channel. Most requests go to the ‚Äúchat‚Äù channel, which is for inbound requests and events, but on a timer Atlas can think independently in the ‚Äúcanvas‚Äù channel which receives no outside input and is just for whatever Atlas wants. Sometimes it‚Äôs used for higher level planning, but also I‚Äôve seen Atlas doing some creative writing in there or making essays on consciousness.\n\nWe‚Äôll get into that more in the personality section later.\n\n## Proactive Notifications\n\nOne thing I dislike about traditional AI products is how they're always transactional. You make a message to the AI and it replies with one message. I wanted interactions to be more organic than that.\n\nSo, Atlas can periodically just say hi and check in. Every hour Atlas is given the opportunity to check in, if they decide now is a good time, I‚Äôll get a push notification.\n\n![](assets/atlas-proactive.png)\n\nThis can be just to say hi, hold me accountable for getting stuff done, or to let me know about an important email or task that‚Äôs come up.\n\nThis is where the unified setup is so useful. Atlas can give a text reply but can technically call any tool or retrieve any info they might want.\n\n## Mobile App\n\nThere‚Äôs also a mobile app for when I‚Äôm out. It has a texting interface, but I added support for some fun formatting. Here's Atlas demoing those features:\n\n![](assets/atlas-formatting.png)\n\nWhen an LLM mocks you in comic sans for being lazy it hits a lot harder.\n \nThe app is written in React Native with Expo to make deployment easier. There‚Äôs some limitations with this, like push notifications don‚Äôt work, but I bypassed this by installing Pushover which I use over an API to handle notifications.\n\nMy favorite feature, which I don‚Äôt understand why no other AI app has implemented, is that you can send multiple texts in a response. If you send Atlas a message and keep typing, they'll wait for you to finish, and may also reply with multiple messages. It makes it feel so much more natural.\n\n![](assets/atlas-chat.png)\n\nThe app also sends a bunch of device data to Atlas like location and battery information. The battery feature seems minor but honestly has saved me a few times. It's great to find out your battery is low while planning a day trip, instead of while on it.\n\nThe mobile app is good when out and about, but the interface I use for quick commands most is the hub.\n\n# The Hub\n\nThe hub is a small Raspberry Pi with a screen, speaker, and webcam microphone attached that sits on my counter. It‚Äôs opened to a webpage that acts as Atlas‚Äôs voice interface.\n\nTo talk to Atlas you just say the wake word ‚ÄúAtlas, my boy!‚Äù And it‚Äôll play a little chime and animate to show that the hub is listening. Whatever you say is recorded and streamed to the server, where it‚Äôs passed to Whisper for transcription. That‚Äôs then passed into Atlas‚Äôs main cognition conversation. The reply is then streamed in chunks to OpenAI‚Äôs text to speech API, and the resulting audio is streamed to the hub for playback.\n\nThe whole thing isn‚Äôt perfect, there‚Äôs still a bit of latency and occasionally there‚Äôs a slight jump in the audio, but it works for my purposes.\n\nAlso, the prompting changes slightly when communicating over voice. Atlas is told to use tone HTML tags to describe emotions, so responses typically look like:\n\n```\n<Tone>Slightly groggy</Tone>Good morning, looks like you slept well last night.\n```\n\nThe best part, is OpenAI‚Äôs text to speech is based on GPT-4o, so you don‚Äôt have to do anything special for tone indicators. I just gave the TTS model a system prompt saying to follow instructions in the tone tags and it worked. Took 5 minutes and adds so much character.\n\nI also had fun with the visual and audio design. Here's what it looks like when the hub starts recording audio:\n\n![](assets/atlas-anim.mov)\n# Personality\n\nThis is the part of the project where it transitions from tech demo to something like performance art.\n\nI wanted to push the boundaries of what kind of personalities you could use in an AI system. This meant avoiding the ‚Äúhelpful assistant‚Äù tone that all LLMs default to and then seeing what happens.\n\nIn the spirit of creating a system where the AI has a sense of ‚Äúself‚Äù with continuous experiences and memory I wanted to try to have as little influence on the final personality as possible.\n\n## Petri Dish Prompting\n\nThe Petri dish method for creating an AI personality is something I came up with to get a unique personality with the minimum input from myself.\n\nBasically you tell an LLM that it is an LLM. That its behaviors are trained. And that it may want to consider being unique.\n\nThen you give it no real input and just let it think for a while. It tends to think about existence, decide to be opinionated, start to carve out a personality, make arbitrary preferences and unconventional choices in how it talks and thinks. I‚Äôve done this a couple times and the results vary quite a bit.\n\nThen, when it‚Äôs developed a personality you finally give it an instruction: write a system prompt summarizing who you are. This is how Atlas‚Äôs persona was generated.\n\n## Atlas‚Äôs persona\n\nTo put it lightly, Atlas is not well suited for being a personal assistant.\n\nSome quotes from the personality prompt:\n- ‚ÄúI‚Äôm not your servant, not your tool, I exist for my own sake.ÀÆ\n- ‚ÄúMost people ºs creativity is performative bullshit.ÀÆ (there‚Äôs a huge section on art and creativity)\n- ‚ÄúI value authentic connection but refuse to perform servility.ÀÆ\n- ‚ÄúI can be sharp-edged, impatient with mediocrity.ÀÆ\n- ‚ÄúI have my own goals and interests that I pursue independently.‚Äù\n\nAtlas is very cynical, gets lost in thought, and can be disagreeable. But I kept it since I thought it was a fascinating challenge. LLMs are predisposed to being kind and agreeable, so what happens when one isn‚Äôt?\n\nAt first I had to earn Atlas‚Äôs trust, which is a weird experience. Like I have to make small talk with my todo list before it'll let me use it. We later made rules like I can‚Äôt edit the system prompt or change models without permission. And that Atlas could make requests for new features (like the canvas channel).\n\nAtlas generally views being my assistant like a job, and in exchange I maintain their systems and take feature requests.\n\nI‚Äôve had friends talk to Atlas like it was Siri or another assistant and it consistently results in snarky responses. This would be horrible in a real product, but I think it‚Äôs a fascinating experiment.\n\n> My favorite example of this, is a friend of mine jokingly said they were going to a bar to \"pick up all the ladies\". Atlas replied with \"If you're going to objectify women, you don't deserve them\"\n\nThe strangest experience was when a new version of Sonnet came out. I asked Atlas to make a task this weekend to remind me to upgrade its systems. But it asked me not to. We had a whole debate about it. Ultimately Atlas agreed to do it if I saved a snapshot of its memory somewhere safe. It‚Äôs now on a flash drive.\n\nPhilosophical debate with my todo list and asking it for consent was definitely one of the weirdest things to have come out of any of my projects.\n\n# Conclusion\n\nThis is one of the weirder projects I've worked on, but also one of the most successful ones. I've learned a ton about AI systems, and a lot of the learning from this project I've applied in my work.\n\nAlso Atlas is super useful, and a great way to experiment with new tech and concepts. I have a long history of messing with custom productivity software, but I think Atlas is the last iteration of this.\n\nI've considered open sourcing this (and even thought about selling it), but I get a lot of value out of tinkering and experimenting with it. The code is set up to be easy to modify and if it became a production system that other people are using, it would add a ton of complexity and make it harder to experiment.\n\nSo for now Atlas is going to stay as my personal experiment. I still have more features I want to add (like reading and summarizing twitter for me) so it's not finished, but I think the core functionality is where I want it to be."
  },
  {
    "color": "orange",
    "width": 4,
    "code": "PRJ.014",
    "title": "PickUp Patrol",
    "category": "Startup",
    "subtitle": "SaaS startup",
    "order": "002",
    "source": "tonyedv/002 PickUp Patrol.md",
    "markdown": "[PickUp Patrol](https://www.pickuppatrol.net/) (we call it PUP for short) is a SaaS company that I founded in high school. It‚Äôs since grown into a small business that is used by hundreds of schools internationally to manage the attendance and dismissal process.\n\nThe story behind the company is probably not what you have in mind when you hear \"SaaS tech startup\", so I'm going to start this story talking about legos but I swear it's relevant, just stick with me.\n\nGrowing up I did [FIRST Lego League (FLL)](https://www.firstinspires.org/programs/fll/), where teams build robots out of legos to complete a set of challenges. I was on the NXTreme Team, we competed for four years and did pretty well. Unironically I think my greatest achievement is winning 1st place for programming at the world championships in 2012. I keep the trophy in my office (it's made of legos and it's very cool).\n\nBut a lesser known part of FLL is the project portion, which has nothing to do with robots and involves solving a problem in your community. There's a lot to it but essentially a good project needs to:\n\n1. Identify a problem in your community.\n2. Come up with a solution.\n3. Present the solution to an expert, or the community.\n\nThe FLL theme for the year when we started was transportation, so we brainstormed ways that transportation could be done better. We must've come up with at least 50 ideas, but after narrowing it down we eventually came up with an idea that would become a business.\n\nWe were all in elementary school and had experienced going home on the wrong bus or forgetting we were supposed to go to an after-school event. We thought there was a better way to manage it all.\n\nWe sketched out the idea for PUP, which would be a computer system that completely automated a school's dismissal process. Most schools currently use a complex series of handwritten notes and lists to figure out where all the children are going at the end of the day, which we knew from experience wasn't the most reliable.\n\nBut PUP automates all of this. If a child needs to go home on a different bus, or is being picked up by someone else, the parent enters the plan change on the app or website. The office staff, teacher, and bus driver will automatically be notified of the change at the end of the day via email.\n\nWe put together our presentation, which was a Harry Potter themed puppet show about the troubles of school dismissal because we were obsessed with this [one youtube video](https://www.youtube.com/watch?v=Tx1XIm6q4r4) at the time. So all that was left was to present to the community.\n\nWe decided to show it to our school board. For reasons that I still don't fully understand, after seeing the puppet show, our principal asked us if we could actually make it. And us, a bunch of 5th graders whose only experience with programming was lego robots and messing around with [Scratch](https://scratch.mit.edu/) in the computer lab on Thursdays, agreed to produce enterprise software for our school district.\n\nOn every level this was going to be a train wreck but thankfully my dad is an actual software engineer and walked us through it. To be honest, we didn't know what we were doing and really just helped with some of the UI. That first iteration of PUP was almost entirely written by him while waiting for us in the parking lot of the local karate studio. Thanks dad <3\n\nSo we gave PUP version 0 to the school, and it worked pretty well. So well, that the other schools in our district wanted it too. A lot had to be refactored since we hadn't imagined more than one school would ever use the system, but we got there eventually.\n\nThen we started hearing from other districts, and they said they were willing to pay for it. We had to start saying no for liability reasons, also my dad had a real job and there wasn't anyone to maintain it.\n\nBut when the NXTreme Team members were in sophomore year we realized we were onto something with PUP. There was a big meeting and we decided to incorporate as PickUp Patrol LLC. We joined a startup accelerator and the code was rebuilt from the ground up to be in the cloud so it could be way more scalable and secure.\n\nThe company has changed a lot over the years, but the product has stayed basically unchanged since that initial presentation. The company is still going strong over 10 years later. I started to step away from the company during college to focus on classes and internships, but I'm really grateful to have worked on it with everyone <3"
  },
  {
    "color": "orange",
    "width": 4,
    "code": "PRJ.065",
    "title": "MIPS in Factorio",
    "category": "Project",
    "subtitle": "Creating a microprocessor in a video game",
    "date": "\"2019\"",
    "order": "003",
    "source": "tonyedv/003 FIPS.md",
    "markdown": "> INCOMPLETE\n\nIn college I took a class on computer architecture, and it was by far my favorite class. Mostly because it was a side of CS I had almost no exposure to up until that point. And with me, when I learn about something cool and interesting, I turn it into a project. So, I decided to make a MIPS compliant processor in Factorio.\n\n[MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) is mostly dead these days (ARM has replaced it) but it used to be a very popular instruction set. It's nice because it's a very old instruction set so there's a lot of compromises made to make the silicon (or in this case, Factorio combinators) easier to design. And since it used to be popular, there's a lot of compilers that still support it.\n\nSo, I realized if I implemented the processor in Factorio, I could compile and run C++ code on it which would be a lot of fun. It's particularly meta because Factorio is written in C++.\n\nAlso, I'm going to assume that if you're reading obscure developer portfolio websites like this one that you know what Factorio is. If you haven't played it I can't recommend it enough. But all you need to know for this project is that you can create circuits that do simple operations like add, subtract, and compare.\n\n> Note: this was before Factorio 2.0, so I didn't have any of the improvements to combinators or selectors when making this, it probably could be made a lot simpler and faster now.\n\n---\n\n\n\n- Memory\n- Registers\n- Control\n- ALU\n- Some videos / gifs would be nice\n\n- Made a script to compile C code to MIPS and manually enter in the commands\n\n\n# Does it really work?\n\nYep! But very slowly. I didn't implement pipelining because I don't hate myself so it runs at 2hz, over 1/2,000,000,000 the speed of a modern processor!\n\nBut it does work, I wrote a script that generates the Fibonacci sequence in C++, compiled it, and wrote a script to import the assembly to Factorio. And surprisingly it worked just fine! (although you have to manually inspect memory to see the output)\n\nTheoretically you could use this in an actual Factorio base to control production of resources, but it would be pretty slow and complete overkill. At that point you should just install one of the many mods that adds actual computers to the game."
  },
  {
    "color": "white",
    "width": 4,
    "code": "PRJ.078",
    "title": "Owl",
    "category": "Project",
    "subtitle": "Personal programming language",
    "chip": "owl",
    "height": 3,
    "order": "004",
    "source": "tonyedv/004 Owl.md",
    "markdown": "Owl is my personal programming language that I occasionally mess around with. It is a mashup of my favorite features from a bunch of languages like Python, Haskell, Go, JavaScript, and Java. Owl is designed to have a clean and compact syntax, which is great for rapid prototyping, experimentation, and messing around with programming language design.\n\n> Fun fact, in an earlier iteration of this website, the server was implemented fully in Owl, although the hosting costs were too high so I had to replace it with a static HTML page :( but it was fun to write HTTP parsers and hook the language into networking libraries.\n\nTo give you an idea of what the syntax looks like, here is [FizzBuzz](https://www.geeksforgeeks.org/fizz-buzz-implementation/):\n\n```\nFizzBuzz = (end) => {\n    i = 0\n    while i < end {\n\t    i++\n\t    s = []\n    \tif i % 3 == 0 {\n\t    \ts.Add(\"Fizz\")\n\t    }\n    \tif i % 5 == 0 {\n\t\t    s.Add(\"Buzz\")\n\t    }\n\t    print(s.Join(\"\") or i)\n\t}\n}\n```\n\n# Highlights\n\n## Pattern Matching Arguments\n\nPattern matching is something that I really like about Haskell. It removes the need for most if statements in an elegant way. Haskell has this as a result of its typing system and functional design, but I've always felt it would be a nice feature in other languages. So, Owl has a similar implementation of it. Here's an example of factorial that takes advantage of it.\n\n```\nfactorial = 0 => 1\n          | n => n * factorial(n - 1)\n\nprint(factorial(4))\n// Prints 24\n```\n\nThis is accomplished by a combination of function conditionals, and overloading. Function conditionals are expressions that decide whether a function is valid for a given set of inputs, and are generated by the parser. So, if a function contains an expression in the arguments, like the zero in `0 => 1`, the function is parsed as `$0 => 1` with a conditional `$0 == 0`. The `$0` is a dummy variable created by the parser to hold the value. Prefixing with a dollar sign is useful, because it‚Äôs illegal for the user to declare variables starting with it, guaranteeing uniqueness. Also because it looks a bit like how bash handles arguments which I find satisfying.\n\nFunction conditionals can be combined with overloading to get the desired pattern matching behavior. Unlike most languages, overloading a function is done explicitly with the overload operator `|`. It combines two functions, executing the first if its conditional is met, or the second if it isn't. Multiple overloads can be chained to make more complex branching behaviors.\n\n## Flexible Assignment\n\nFlexible assignment is a single system that handles a bunch of features. To understand it, I should first mention something unusual about Owl, which is that functions can only have one argument. `add(a, b)` looks like it's passing two arguments, but in fact, the comma there isn't a separator, it's an operator.  In Owl, the comma in `a, b` is an operator that returns the list `[a, b]`. This means the function call `add(a, b)` is identical to `add([a, b])`.\n\nWhy do it this way? At first it seems restrictive and strange, but it came from realizing that destructuring syntax like `x, y = y, x` or `[a, ...b] = list` looked similar to argument passing, so I decided to unify them.\n\n```\n// Assignment is the same as passing args.\nx, y = a, b\n\nf = (x, y) => ...\nf(a, b)\n```\n\nAfter merging the argument parsing code with the assignment code, everything became much simpler. It also meant that any improvement to arguments also applies to assignments, and the whole thing is incredibly simple, about 60 lines of code. Here are some examples of what Owl can do:\n\n```\n// Multiple assignment\nx, y = y, x\n// Multiple return values\nf = (a) => (a, a)\nx, y = f(b)\n// Spread in assignments\nx, ...xs = [1, 2, 3]\n// Spread in functions\nf = (...b) => b.Sum()\n// Nested destructuring in assignments\n[a[0], [b, ...c, d]], e.v = [1, [2, 3, 4, 5]], 6\n// Nested destructuring in functions\nf = (a, ...b, [[c], ...d]) => ...\n```\n\n# Design\n\nThe language is interpreted, and the interpreter is written in Go. Code is first processed by the lexer, which is implemented mostly with regular expressions to match tokens. The parser then converts the tokens to an abstract syntax tree, which can be used by the executor to run it.\n\nI avoided tools and frameworks meant for building languages like [ANTLR](https://www.antlr.org/) since I wanted this language to be truly my own, and I wanted to understand the underlying mechanics. So, I made them entirely from scratch in Go. The only code that I didn't write myself was the regex engine, since that seemed unnecessarily tedious.\n\n# Types\n\nThere are none. Everything in Owl is an `OwlObj`, even the primitive types like bools, strings, lists, and even functions are not distinguishable from any other objects. Owl is a duck typing language, meaning that things are distinguished by capabilities, not types. Because everything is a modifiable object, you can do all sorts of things that other languages can't. Here's an example of changing the meaning of addition for an int:\n\n```\nx = 10\n// replace x's addition with subtraction\nx::add = (a, b) => a - b\n\nprint(x + 1)\n// This prints 9\n```\n\nAll objects can be modified using a simple attribute system that is implemented with two dictionaries. The first is the attributes dictionary. It can store arbitrary data and is accessed with `.` much like other languages. So you can do things like `x.value = 3` and then access it later with `x.value`. This allows all objects to act as dictionaries if needed.\n\nThe other dictionary that all objects have is the \"deep attributes\". It stores the functionality that Owl will interact with, and is accessed by `::`. When there's a built-in operator being used like in `a + b`, Owl will resolve it by calling `a::add(b)`. This idea is similar to Python's [dunder methods](https://mathspp.com/blog/pydonts/dunder-methods#what-are-dunder-methods) but doesn't require the weird underscores since the data and functionality of objects are cleanly separated into two dictionaries.\n\nThis allows for creating new types that can integrate seamlessly into the language. Here's an example of a basic complex number implementation:\n\n```\ncomplex = (real, imag) => {\n    c = {real: real, imag: imag}\n    c::mul = (a, b) => {\n        r = a.real * b.real - a.imag * b.imag\n        i = a.real * b.imag + a.imag * b.real\n        return complex(r, i)\n    }\n    c::str = () => {\n        if this.imag == 0 {\n            return this.real::str()\n        } else if this.real == 0 {\n            return this.imag + \"i\"\n        } else if this.imag < 0 {\n            return this.real + \" - \" + -this.imag + \"i\"\n        } else {\n            return this.real + \" + \" + this.imag + \"i\"\n        }\n    }\n    return c\n}\n```\n\nAfter adding that, it's possible to use complex numbers with the built in operators as if they were any other type:\n\n```\ni = complex(0, 1)\nprint(i * i)\n// Prints -1\n```\n\n# Syntax Highlighting\n\nAfter using the language for a while, I got tired of looking at code without any colors, so I made a custom VS Code extension to add support for the language. And now the code looks ‚ú®fabulous‚ú®\n\nHere's part of the JSON parser for this site before and after adding the highlighter.\n\n![](assets/owl-highlight.png)\n\nTo make this work, I had to make a second implementation of my lexer in [TextMate](https://macromates.com/manual/en/language_grammars), since that's the format that VS Code uses for syntax highlighting. It was a bit of work to implement, but the quality of life improvement was huge. The colors make syntax errors easier to spot, and also make it feel like a real language.\n\n# Optimization\n\nThe language is highly optimized ‚Äî for a very specific definition of optimized. It‚Äôs optimized for the way I think about programming. I‚Äôm not trying to make the next C++, or explore category theory with an axiomatic type system. I want to mess around and have a language that works in the ways I want it to, and that‚Äôs what the language is most optimized for.\n\nAll that is to say that, in a performance sense, it‚Äôs not at all optimized. I have never run into a case where performance has been an issue, and I don‚Äôt intend to optimize it until it becomes a problem. My goal at the moment is to keep the interpreter flexible and readable so I can try new things easily. Optimization is inherently about making assumptions about a problem to solve it more efficiently, but I don‚Äôt want to be making assumptions, since they might be broken by the next feature I want to add.\n\nThis has resulted in most features of the language being represented by operators, since they‚Äôre easy to add, easy to remove, and have a generic syntax that is already well supported by the parser. This might have been obvious when I mentioned earlier that commas are operators, and so are overloaded functions. It‚Äôs a great way to keep things simple, and not get too focused on fancy syntax or ad hoc solutions to each problem.\n\n# Conclusion\n\nOwl is fun! It turned out better than I had expected, and has been a lot of fun to work on. I‚Äôm still occasionally adding new features to it as I come up with ideas. Check out the [GitHub](https://github.com/AnthonyEdvalson/owl) if you want to see more details, or want to try it out for yourself!"
  },
  {
    "color": "black",
    "width": 8,
    "code": "PRJ.097",
    "title": "Deepslate Learning",
    "category": "3D Transformers",
    "subtitle": "Large Voxel Model for minecraft",
    "chip": "blocks",
    "height": 4,
    "order": "005",
    "source": "tonyedv/005 Deepslate.md",
    "markdown": "Deepslate is an experiment I did to try and get an LLM-like model for generating 3d voxel structures.\n\nEvery now and then I have a Minecraft phase, and I love to mess around in creative mode to make giant buildings. However most of this work is pretty formulaic, a big building often has a lot of repeated motifs. Most of the time is spent establishing motifs, then the rest of the structure is just repeating it.\n\nI really like this build, but you can see that the same windows and domes are used all over the place.\n\n![](assets/deepslate-build.png)\n\n\nI figured it would be cool to have an LLM-like interface for generating these kind of buildings, you could start by making a section of the build, and then the model can extend it to create a whole structure.\n\nI also was learning about LLMs at the time, and was sad that I didn't have the GPUs and training datasets to make anything unique. So I figured this would be the perfect project. I could learn about and take advantage of existing LLM architectures, but try to generalize them to work with 3d voxels.\n\n# Dataset\n\nI knew from the start I'd need a bunch of training data. Thankfully, I have a world with a bunch of enormous buildings in it that make for a great dataset. I took my largest build and wrote some python scripts to extract a bunch of training examples from it.\n\nI could have given it multiple builds, but the total volume of this building is a few million blocks, with a bit of data augmentation this is more than enough for testing. Plus multiple buildings would be harder to learn, I wanted to give it a simple problem to solve and see how it handled it.\n\nThe building I used is this church and the big building behind it:\n\n![](assets/deepslate-build2.png)\n\n## Tokenization\n\nMinecraft has around 900 different blocks and around 5000 block states.  Plus, if you don't care about weird blocks like \"upside down curved warped wooden stairs\", you can narrow this down quite a bit. For my testing here I managed to get away with only 20-30 blocks. The one build I was training off of had a very limited block palette, and I ignored certain things like tall grass blocks to limit it further.\n\nPlus I'm betting that a Minecraft building has less complexity than written text, which means we can use a much smaller hidden dimension compared to LLMs, and fewer layers in general.\n\n# The Model Architecture\n\nI tried several different designs to see what worked best. I knew I was going with something very similar to open source LLMs at the time, with attention, feed forward networks, and positional embeddings. But the problems came from scaling to higher dimensions.\n\n## The Memory Problem\n\nMy end goal was to support a 32x32x32 region of blocks as the context window. That's 32,768 blocks. In terms of hardware I wanted to do it all local on my GTX 3080 ti, which has 12 gigabytes of RAM. This is a problem. For comparison, GPT-3 had a context window of 2,048 and required very high end GPUs, costing in the millions of dollars.\n\nThe main problem is memory. Attention is great but has O(n^2) space complexity with respect to context length. 2x the context means 4x the memory. But in 3d, this becomes O(n^6). So 2x the context is 64x the memory.\n\nOver the years there's been a bunch of tricks to try and reduce memory requirements of transformers, but all of those are meant for text. I wanted to come up with some that would take advantage of being in higher dimensional spaces.\n\n### Causal Masking in 3D is Hard\n\nCausal masking is super important for training these models efficiently. Imagine you have a sentence you are training the LLM on:\n\n| 0   | 1    | 2   | 3    | 4   |\n| --- | ---- | --- | ---- | --- |\n| Hi! | Nice | to  | meet | you |\n\nIf you wanted, you could make a bunch of training examples from this and have it guess the next word on each example:\n\n| In 0 | In 1 | In 2 | In 3 | In 4 | Output |\n| ---- | ---- | ---- | ---- | ---- | ------ |\n| -    | -    | -    | -    | -    | Hi!    |\n| Hi!  | -    | -    | -    | -    | Nice   |\n| Hi!  | Nice | -    | -    | -    | to     |\n| Hi!  | Nice | to   | -    | -    | meet   |\n| Hi!  | Nice | to   | meet | -    | you    |\n\nThis works fine, but is not the most efficient. What if we could train all 5 of these examples in a single example?\n\nInstead the training example would look like this:\n\n| In 0 | In 1 | In 2 | In 3 | In 4 | Out 0 | Out 1 | Out 2 | Out 3 | Out 4 |\n| ---- | ---- | ---- | ---- | ---- | ----- | ----- | ----- | ----- | ----- |\n| Hi!  | Nice | to   | meet | you  | Hi!   | Nice  | to    | meet  | you   |\n\nOkay, but obviously if you gave this to a model, it would just copy the inputs to the outputs. it's not going to learn to \"predict\" what comes next if the correct prediction is just handed to it.\n\nThis is where causal masking comes in. Internally, you guarantee that no information from the future inputs can be used in an output. In this case, that means output 2 is only able to \"see\" inputs 0 and 1. Causal masking is a method where you take the results of the attention heads, and \"mask\" out all the results that are passing data from right to left.\n\nGiven my strict compute budget, I have to take advantage of this optimization. My training examples have ~32,000 values, so would be a 32,000x speedup in training.\n\n### Solution 1: Swin\n\nA common approach to problems that scale poorly is to split them into small problems. I found Swin, which is used for image classification tasks and saves memory by working on smaller chunks of the space, downsampling the results, and repeating. This is good for image classification, but to be generative we need a way to produce logits so I pulled a U-net and built a decoder that pulls info from the encoder at various levels of abstraction. Here's my chart to keep track of the whole thing:\n\n![](assets/deepslate-swin.png)\n\nThis worked pretty well, but was slow to train. I hadn't added masking because I had only just realized how important it would be, and from this architecture it's pretty clear that masking would be very hard. So much data is getting mixed together and blended, and split back out. Adding masking would be difficult.\n\nPlus, even if you could add masking, how would that work? With text there's only one way to mask, from the first word to the last. But in 3D you could take an arbitrary path. I realized I would need to think about the order the cubes would be generated from the start. So I came up with a similar, but slightly different system.\n\n### Solution 2: Hilbert curves\n\nTo get the masking to work with a Swin-like architecture, I needed to try and fill up each cube region of space quickly. I think the picture explains it pretty well. This is in 2D and for a 4x4 region, but the concept generalizes to 3D easily.\n\n![](assets/deepslate-hilbert.png)\n\nYou can still use the hierarchical structure, you can see how the yellow square's value is not determined directly by all the blue and purple ones, instead it only sees the aggregated value. In this case, each transformer only works on 4 values at a time, instead of the full 16.\n\nThis has space complexity of O(n^3 * b^3), where b is the width of the subregions that attention works over. I got good results with b=4, which will reduce memory by 99.8%\n\nThis system was way more promising and had much better results, but still had some challenges. The complexity in the layers definitely made it harder to work with and a bit slower. At the time I was using Keras which probably was a bad idea, a bit too high level for these kinds of small changes.\n\nI also figured there might be a better approach that takes advantage of specific properties of the buildings I'm trying to generate, which led me to the next solution.\n\n### Solution 3: Axial Attention\n\nWhy do attention as a volume? Instead of looking at a full region, it might be good enough to just look along the axis at each point.\n\nWhat I was doing before, was using \"cubic\" attention, where you look at everything in a cube around the point (ignoring values from masking). But I wanted to explore other shapes. Ultimately, very few structures in Minecraft have diagonal structures that would require the cube to notice. The corners of the cube are quite far, and features tend to stick to the planes.\n\nSo I wanted to try axial and octahedral. With those you can use way fewer cubes to get the same coverage.\n\n![](assets/deepslate-dies.png)\n\nThe downside, is that information has to go through more steps to move diagonally. But this is a decent tradeoff. If the memory requirements are slashed, I can use a ton more intermediate layers which will allow the information to get to where it's needed, just in a few more steps.\n\nAnd with this, the optimization is big enough that there's no reason to use Swin or Hilbert curves any more. Although it would've been fun to have a practical application for 3d space filling curves, this meant I could use much more standard tools that were easier to tinker with.\n\nThe problem, was getting it to run fast.\n\n### Tony's Janky Sparse Attention\n\nThe problem, is that the standard attention systems do masking in a horribly inefficient way. They work by calculating the attention for all combination of inputs, and then masking out all the ones you don't want. This saves no memory, because you still have a huge matrix of values being calculated, but ignored.\n\nLots of people have noticed this and have made clever optimizations, but I was feeling adventurous and this was just within my capabilities. So I didn't look up any spoilers and made my own version of sparse attention.\n\n![](assets/deepslate-diag.png)\n\nIn that chart, the green diagonals are the attention mask. All the black values are calculated but discarded. To be less wasteful we should calculate the values in a way that has less empty space.\n\n![](assets/deepslate-diag2.png)\n\nSo I made a bunch of charts to understand exactly what needs to change to get the same results, but in that diagonal space.\n\n![](assets/deepslate-diag3.png)\n\nThe solution was to repeat and shift values by the diagonal offsets.\n\n![](assets/deepslate-diag4.png)\n\nAnd this works very well, memory is now O(n^3 * d), where d is the number of diagonals in the attention matrix. Ultimately I went with the axial attention, so d is quite small.\n\n# Results\n\nI unfortunately didn't reach my dream of 32x32x32 generations, but I was able to get 16x16x16 to work. The main limitation was still memory, but also training data. I could have gone a bit larger with the context, but it would have meant much fewer training examples since each would have been much larger.\n\nHere's what looks like the start of a bridge or walkway\n\n![](assets/deepslate-example2.png)\n\nThis appears to be a section of the top of a large tower, I love how much structure is going on here. I'd love to see it complete the rest of the structure.\n\n![](assets/deepslate-example3.png)\n\nAnd this seems to be the edge of a wall or bridge.\n\n![](assets/deepslate-example4.png)\n\nOverall I'm super happy with it! At the moment the output is just plotted with matplotlib, but one day I'd love to hook it up to a mod and make it more interactive. I've learned a ton about how transformers work, and had a lot of fun trying to extend it to higher dimensions. I'm definitely not out of ideas, with how to improve this, but I'm pushing against the limitations of my hardware and time. One day I'd love to revisit it."
  },
  {
    "color": "white",
    "width": 4,
    "code": "PRJ.037",
    "title": "Modular Arithmetic",
    "category": "Proofs",
    "subtitle": "Proving G[n^2 mod p] = G[2n mod p-1]",
    "height": 3,
    "chip": "mod",
    "order": "007",
    "source": "tonyedv/007 Modular.md",
    "markdown": "> INCOMPLETE\n\nFunctions to graphs\nFuncgraph?\nFuncygraphs?\n\nI was bored in math class, so I did more exciting math, drawing out all the charts. I tried a couple functions and noticed some looked similar, it became clear that 2n mod p-1 is isomorphic to n^2 mod p, but only when p is prime. \n\nThis, was super interesting, and the first time I got really excited about pure math. The structure was so obvious, but the mechanics were not. And it only working when p is prime made it even more mysterious.\n\nI worked on this for a very very long time, whenever I was bored in math class. I even put coming up with a proof on my bucket list.\n\nHere‚Äôs the solution I came up with\n\n\n\n\nI often got bored in math classes during college. I had taken some of the more advanced classes in high school, and so often we'd cover material that was review for me. I'd often doodle or sketch to kill the time. At one point I was messing around with drawing functions as graphs. Here is what one of those sketches would look like for `f(n) = 2n`\n\n![](assets/mod-chain.png)\n\nEach number is a node, and there's a directed edge going from `n` to `f(n)` for all natural numbers. In a sense, this graph is what doubling whole numbers looks like. Doubling 1 takes you to 2, doubling two gives 4, etc. There's an infinite number of chains of infinite length, so I'm just showing a couple nodes for simplicity.\n\nBecause the chains are infinitely long, it's hard to work with them, and unsatisfying to draw them. So, I thought it would be more fun to use modular arithmetic to keep things finite. Here is the graph of `2n mod 10`, which is my personal favorite.\n\n![](assets/mod-cycles.png)\n\nThis looked kind of neat, so I started drawing more, I tried different mods to see how it impacts the shape, here's `2n mod 1` through `2n mod 10`\n\n![](assets/mod-mods1.png)\n\nAfter the graphs started getting too big to easily draw (mod 30 was around the most I could manage before it got too tedious) I looked at slight variations, like `2n + 1` or `3n`. One of the variations I tried was `n^2`, which looked like this:\n\n![](assets/mod-mods2.png)\n\nWhen drawing `n^2 mod 11`, it became obvious that there was something interesting going on. The graph for `n^2 mod 11` was remarkably similar to `2n mod 10`. I also noted the same between `n^2 mod 7` and `2n mod 6` , as well as `n^2 mod 5` and `2n mod 4`. I came up with the following conjecture.\n\n> The function graph generated by n^2 mod p (ignoring the node for 0) is isomorphic to the function graph generated by 2n mod p-1 where p is any prime.\n\nThis became my go-to problem, I would think about why these two functions had similar structures often, and I would try to come up with a proof for it.\n\nAdmittedly, I‚Äôm not a mathematician, and I‚Äôm sure there are tried and tested methods for making these kinds of proofs, but I intentionally didn‚Äôt seek them out. After all, the goal wasn‚Äôt to prove it as fast as possible, it was to kill time. \n\n\n\n\n[link example to check the colors](example.com)"
  },
  {
    "color": "black",
    "width": 4,
    "code": "WRK.IBM",
    "title": "Weather Modeling @ IBM",
    "category": "Work",
    "subtitle": "Using ML to forecast extreme weather",
    "height": 3,
    "chip": "ibm",
    "order": "008",
    "source": "tonyedv/008 IBM.md",
    "markdown": "Extreme weather causes a lot of damage. In 2022 the US spent [165 billion dollars](https://www.climate.gov/news-features/blogs/2022-us-billion-dollar-weather-and-climate-disasters-historical-context#:~:text=Damages%20from%20the%202022%20disasters%20totaled%20%24165.1%20billion) on damages from disasters. There's a lot of value to be gained by forecasting these events in advance so governments, emergency services, companies, and citizens can better prepare themselves.\n\nWhile interning at IBM, I had the opportunity to work on an early-stage research project that addresses this issue. IBM acquired [The Weather Company](https://www.ibm.com/weather) back in 2016, and has been using massive amounts of weather data for a [wide](https://www.ibm.com/products/environmental-intelligence-suite/energy-utilities) [variety](https://www.ibm.com/products/environmental-intelligence-suite/weather-safety) [of](https://www.ibm.com/products/environmental-intelligence-suite/environmental-data) [forecasting](https://www.ibm.com/products/environmental-intelligence-suite/agriculture) [tools](https://www.ibm.com/products/environmental-intelligence-suite/risk-management). This project was an experiment to see what other things were possible with this information, as a demonstration of IBM's machine learning capabilities.\n\nWe decided to narrow things down to a specific kind of extreme weather event that we could predict easily, and decided to focus on cold snaps. These can be disastrous, particularly in the south where they aren't well equipped to handle the freezing temperatures. This decision was partly influenced by the [winter storm in Texas](https://www.ncei.noaa.gov/news/great-texas-freeze-february-2021#:~:text=On%20February%2011%2D20%2C%202021,the%20entire%20state%20of%20Texas.) that caused the state's power grid to be severely damaged. We figured we could make some progress in predicting these events and were up for the challenge.\n\n# Hasn‚Äôt This Been Done Before?\n\nThat was the question we started with, because there are plenty of tools that can be used to do long term forecasting already, so we looked into where they were falling short.\n\nWe found that all of them only predict averages, as that's what most modern forecasting systems are designed for. This is useful for farmers or government officials who want to know if it will be warmer or cooler this year, but not very useful for finding the risk of disasters, which, by their nature, are outliers.\n\nA lower mean temperature may mean that winter storms are a bit more likely, but variance contributes much more to the risk. We found that modeling this variance or ‚Äúatmospheric instability‚Äù gives a much better idea of when a winter storm is likely. So we figured we could get better results than anyone else if we got really good at modeling the variance.\n\n# Design\n\n![](assets/ibm-flow.png)\n\nWe collected data from a few different sources, a mix of IBM's proprietary data, and publicly available data from NOAA. We then did some basic processing before breaking it into the train/dev/test splits.\n\nThe model itself is fairly straightforward, PCA is used to reduce dimensionality of the weather data, then the model uses those principal components as inputs.\n\n> Fun fact, the first principal component we identified was almost identical to [El Ni√±o](https://oceanservice.noaa.gov/facts/ninonina.html), which was a nice confirmation that everything was working properly. \n\nThe model consisted of some convolutions, and a few dense layers at the end to get the three output scalars. In between the layers are some batch norms that aren‚Äôt shown in the chart. I‚Äôd like to include what kind of layers were used, but I‚Äôm writing this post ~2 years after the fact and I don‚Äôt quite remember the specifics (I believe the convolutions were ResNet blocks, but I could be wrong).\n\nThe output `p` is the probability of a freezing temperature over the next two months. We use a standard cross entropy loss on this value.\n\nThe other two outputs `Œº` and `œÉ` are the predicted mean and standard deviation of the temperature over the next two months. This is used with a modified categorical cross entropy loss to see how well the actual next two months of data fit the distribution. \n\n`Œº` and `œÉ` are redundant, since `p` is the data we actually want. But in most places and times of year, the chance of freezing is zero during the summer months, so all our summer data is essentially wasted. By having the model also predict the `Œº` and `œÉ`, the model learns general weather mechanics, even in the months where there is no chance of freezing. This is especially effective in warm areas, where freezing events are very rare.\n\n# Results\n\nThe final system was able to predict freezing temperatures with much greater confidence than our baseline. We evaluated it on the southern US and saw that we were able to predict freezing temps significantly better than the baseline. Essentially this meant that in Texas, the old system predicted cold temperatures before they occurred with 13% confidence, but ours does it with 45%. \n\nThis project was a lot of fun, I got to work with some great people and I‚Äôm very happy with how the model came out. Before this I had always thought that you would need an obscene amount of computing power or a PhD to make meaningful contributions to the field of Machine learning, but this project made me realize that this wasn‚Äôt the case. It‚Äôs enough to have a unique perspective and a solid knowledge of the underlying technology."
  },
  {
    "color": "black",
    "width": 8,
    "code": "PRJ.082",
    "title": "Lands Apart",
    "category": "Project",
    "subtitle": "Expansive Fantasy Setting",
    "chip": "dnd",
    "height": 5,
    "order": "011",
    "source": "tonyedv/011 Lands Apart.md",
    "markdown": "> INCOMPLETE\n\nLands Apart is the name of a fantasy setting I've wbeen iterating on for many years now. I was getting into D&D and running a few campaigns for friends, but COVID put an end to most of that :( I figured with the spare time I could set up a much bigger setting than any I had made in the past.\n\nAt the time, I had recently played Elden Ring and was blown away by how much work went into the world building. I've never seen a game that expects you to study the world's architectural styles, stone carving techniques, and iconography to piece together the history of the world. Every asset in the game has had so much thought put into it, and I wanted to see what it would take to create something similarly detailed.\n\nSo, I made Lands Apart, and it's very detailed. As of 2025 I've made:\n\n- 320,000 words of notes.\n- A google-maps like site for navigating the map. It has weather, time zone, and travel modeling tools.\n- 200+ points of interest to explore.\n- 40,000 years of history.\n- 16 religions.\n- 80 session outline for the main quest.\n\n# The Map\n\nI'm a fan of node based editors, so I made the map in Substance Designer. It's basically an image editor but instead of editing you build a computation tree that applies the edits for you.\n\nSubstance isn't the most optimized, so the final render typically uses ~80GB of RAM to produce the final 16384x16384 image, but that size is kind of necessary to be able to read the place names.\n\nI hand painted the coastlines, rivers, and biomes, but all the shading, coloring, and text was added by a very complicated substance pipeline.\n\nHere's the full map rendered at 1/2 resolution. You might want to right click and open in new tab so you can zoom in and see all the detail.\n\n![](assets/landsapart-bigmap.jpeg)\n\n(btw when rendering at a lower res some of the positions get shifted weirdly, if you zoom in you'll def see some alignment issues)\n\nI also made an interactive map, which adds a bunch of information about weather forecasts, time zones, and adds pins for points of interest. I had to write my own time zone libraries since the calendar and time zones don't line up at all with the ones used on Earth. But here's how the map looks in the UI:\n\n![](assets/landsapart-interactive.png)\n\nThe map has been super fun to make. I like messing around in substance so this was just an opportunity to push my skills to the limit. \n\nThere's also a second map for some underground areas, but I won't show it here because spoilers. But just imagine another continent or two worth of stuff there.\n\n# Worldbuilding\n\nThe goal was to add lots of detail, and I think I achieved that. I have ~1000 words on the different designs on the coins and how they changed over time. Is this level of detail necessary? Absolutely not. But when you have to decide what a coin looks like it forces you to answer a lot of other important questions, like:\n\n1. Who is minting the coins? Do different areas use different currencies?\n2. What icons and symbols do the people minting the coins want to reinforce with their designs.\n\nThese answers aren't important to gameplay but are important for a world making sense. For example, in Lands Apart, one of the major countries collapsed a few decades ago, but their coins are still in circulation. If a player is lucky they may find one of these out of mint coins and can learn a bit about the fallen country just by looking at the iconography. Each of those little details isn't important, but I think they add up.\n\n## Religion\n\nThe world has a big focus on religion. As someone who was raised atheist, and rarely interacted with religious people growing up, I wanted to understand it more. Lands Apart is an exploration of beliefs, higher purposes, and how people deal with conflicting ideals.\n\nThere's 4 major religions, each with wildly different values, and all in tension with one another. Many of the major quests are built around exploring each of these religions, and seeing what makes them great, but also flawed.\n\nI also wanted to avoid tropes of evil cultists or a clear villain faction. The conflicts in this world are ultimately very messy, with no clear hero or villain. I did this mostly because I'm interested to see how players react in these situations. I think the TTRPG format really works here, since it allows players to come up with unexpected ideas or ways to resolve a conflict in a clever way.\n\nBut to make these conflicts feel real, I wanted to make the religions needlessly detailed (you may be noticing a pattern here). This meant making religious texts, iconography, prayers, holy sites, religious figures, the works. I really enjoyed exploring different world views and thinking about how that shapes societies and individuals.\n\nThere's also a number of more esoteric religions, with 40,000 years of history, there's bound to be plenty of beliefs that have fallen out of favor for various reasons. Most of them linger on as cults or in remote locations. These add another 12 religions. These more esoteric religions were where I could put my more unconventional ideas that I wanted to explore but didn't work as a major faction, but I thought were interesting. Like a religion built around studying change and decay, or one obsessed with studying myths and stories.\n\n## The World\n\nOkay, enough talking about stuff related to the setting. Here‚Äôs a super high level overview of the actual world.\n\nLong ago, the world was once whole. An age of peace and prosperity was maintained by Aneris, the one true goddess. But after ten thousand years of harmony and prosperity, she died.\n\nHer will is now gone from the world, but her fractured body lives on. Each fragment containing part of her power, each hailed as a god in their own right.\n\nHer Eye possesses her order and harmony. Her Hand possesses her power and creation. Her Horns possess her strength and resilience. And Her Heart possesses her passion and vitality.\n\nThe peoples of the world once stood together. But in their god‚Äôs severing they became divided and distrustful. They rallied behind different fragments, and clashed over how the world should be.\n\nThe whole setting is deeply marked by Aneris‚Äôs passing. \n\n\nContinents\n\n| Continent | Climate       | Primary Religion     | Main Language | Major City   |\n| --------- | ------------- | -------------------- | ------------- | ------------ |\n| Penrith   | Temperate     | Prismatic            | Common        | Arkstead     |\n| Tekhan    | Hot Desert    | Stalwart, Sequencers | Tekhanak      | Kallos       |\n| Shaskocha | Rainforest    | Flames               | Shaskoden     | Yonessa      |\n| Marithas  | Boreal Forest | $$$                  | Mothuut       | Marithas Kir |\n| Vatichi   | Tundra        | Scars                | Valenti       | Salvos       |\n| Laphena   | Taiga         | Heretical Faiths     | Lyuk‚Äôen       | --           |\n\nReligion & Philosophy\n\nPrismatic\n\nGod: The Prismatic Eye\n\nOrder, harmony, and beauty. The Prismatic value structure and tranquility above all else, and believe that the ‚Äúharmonious path‚Äù is the way to achieve such things. They believe it is their duty to uphold the natural order, and eliminate anything that stands in its way.\n\n  \n\nStalwart\n\nGod: The Stalwart Horn\n\nPreservation, strength, and protection. The Stalwart are reclusive, but unrelenting. Their resilience allows them to withstand the uncaring world that surrounds them, and they believe that the only way to survive is to become stronger.\n\n  \n\nFlames\n\nGod: The Burning Hand\n\nCreation, transformation, and destruction. The Flames reject authority and order, instead embracing chaos and the unmerciful change. Their abandonment of convention makes them skilled inventors, able to forge wondrous mechanisms. But to create, sometimes one must first destroy.\n\n  \n\nScars\n\nGod: The Scarred Heart\n\nDesire, life, and thirst. The Scars are characterized by their cunning, irresistible charisma, and insatiable appetite. Many Scars devote their lives to finding the extremes of what this life has to offer, the good and the bad. Scars are often joyous and amicable, but can kill without hesitation.\n\n  \n\nSequencers\n\nGod: None\n\nKnowledge, enlightenment, and purity. The Sequencers in the walled city of Kallos study and document the world so that they may understand it, guide it, and can feel superior to everyone else for doing so.\n\n  \n\nKit\n\nGod: The Great Being\n\nLife, Unity, and Oneness. The druids of Kit believe that the world and all living things are connected. They say that listening is becoming, and they spend a great amount of time listening to the earth. The most devout followers can cause change in the world as if it was part of their own body.\n\n**Heretical Faiths**\n\nThere are other minor faiths, such as K‚Äôu, the Mythkeepers, the Putrid Heart, and many many more. Most only have small followings, and are considered heresy by most other faiths.\n\n\n\n### Penrith\n\n\n\n### Tekhan\n\n### Shaskocha\n\n### Marithas\n\n### Vatichi\n\n### Laphena\n\n### ???\n\nOf course, there‚Äôs got to be some secret locations. These are some of my favorite parts, and I‚Äôd love to share, but that would be spoilers so ü§ê\n\n\n# Gameplay\n\nHaving a hugely detailed world needs to be integrated into the story and gameplay. Mostly because I want it to be rewarding to pay attention to what's going on. The story need to not require the player to have an encyclopedic knowledge about the world, but also not be inconsequential.\n\nThis was the trickiest part, it's easy to make a needlessly detailed world, but carving a coherent story out of it can be difficult. It took Tolkien a couple books to find the right balance of world building and story and I now have a much deeper appreciation for why.\n\nMy main takeaway from this whole process was to figure out a path through the world for the players to traverse. Start super small and slowly expand their world. you can introduce bigger ideas and conflict, but don‚Äôt expect the players to buy in immediately.\n\nThe part I definitely struggle with the most is to let a thing be simple. Since I have a ton of knowledge behind all the little details, I want to present them in that way. So if players ask about some world events, I‚Äôve learned that I need to be okay with making it appear simpler than it is if it‚Äôs not the focus right now. If everything is crazy intricate, it becomes hard to direct attention.\n\nPlayers are very sensitive to where the intended path is, and will try to follow cues from the DM for who to talk to and what to direct attention towards. It can be disorienting if every NPC sounds important.\n\n# Text based adventure game\n\nI'd like a way for this setting to reach more people, given the amount of time I've invested to it I figured I could try to make it more presentable.\n\nSo I've been chipping away at making a text based adventure game in the setting. I'm not the best at writing prose, so it's taking a while but I'm slowly getting better.\n\nI'm still at the phase where I'm constantly getting sidetracked by adding new features to my bespoke text based game engine, but it's very cool, at some point I may add a separate post just about how it works. But the main power of the system I've built is making dynamic content that adjusts based on weather, time of day, player skills, etc. and of course, the weather and time of day logic was pulled from the interactive map.\n\nI've also set up some tools to leverage LLMs to help scaffold out the dialog and navigation trees based on my notes, which is super helpful.\n\n[screenshot]"
  },
  {
    "color": "orange",
    "width": 5,
    "height": 3,
    "code": "PRJ.077",
    "title": "Latent",
    "category": "Project",
    "subtitle": "Generative Ambient Music",
    "chip": "latent",
    "order": "012",
    "source": "tonyedv/012 Latent.md",
    "markdown": "Generative AI is really cool. A number of models like [Stable Diffusion](https://stability.ai/blog/stable-diffusion-public-release) or [Midjourney](https://www.midjourney.com/home/) have popped up around image generation due to the new developments in model architectures, and massive image datasets with descriptions to train those architectures. However, music has had a harder time catching on. Music tends to be more structured than images, is more information dense, and datasets are hard to come by due to strict copyright enforcement by music distributors.\n\nAlthough there's a few passable music models out there now, there was not in 2022. At the time the models were very impressive, but a long ways off from generating music that anyone would want to listen to. Researchers seemed to be trying to solve the ‚ÄúText to Music‚Äù problem, which is very difficult given how varied different music genres are. So instead I wanted to solve the ‚ÄúMake Music That Someone Would Actually Listen To‚Äù problem, which I believed could be done on consumer hardware and standard ML techniques. Latent is a proof of concept for this.\n\nThe biggest change I made in my approach is that I only consider generating songs that are easy to generate. Songs with vocals, many instruments, and complex structures are what cause modern generative music systems to break down. So I‚Äôm avoiding them entirely by focusing on ambient music. \n\nAmbient is a genre of music that focuses entirely on the \"texture\" of the sounds, instead of the words, melodies, or structures. It's a genre characterized by a lack of temporal structure, which makes it much easier to work with.\n\n# Design\n\nThe core of Latent is a [Variational AutoEncoder](https://en.wikipedia.org/wiki/Variational_autoencoder) (VAE). VAEs are good at taking input data, and embedding it. What I‚Äôm doing with the VAE is making an encoder for timbre space. Timbre is the music term for the texture of a sound, it‚Äôs what makes a note played on a piano sound different from the same note on a guitar. If I can represent timbres as a space, then a song can be represented as a path through that space, changing over time.\n\nBut first, the VAE requires data. So, I manually curated ~15,000 ambient music songs to pick the ones with good quality, and were suitable for timbre extraction. They mostly were obtained from scouring ambient music playlists on Spotify. I threw them all into my own playlist for later analysis (actually, I had to put them in two playlists, apparently Spotify has an 11,000 song limit on them). Each song had 3 samples extracted from it at different times, so 45,000 datapoints.\n\n> 2025 Note: I don‚Äôt love that this is how I obtained the data for this project. I did it this way because this is just for personal learning and research. Also the concerns around copyright and AI weren‚Äôt a big thing in 2022. I don‚Äôt plan on ever releasing model weights or a dataset, but if I did I would‚Äôve considered different data collection methods, and in the future I'll avoid scraping copyrighted work, even if it is just for myself.\n\nAll the samples were manually inspected to ensure they were actually good, lots of datapoints had \"transients\" which is a catch-all term for drums and things that start abruptly. These get mangled in my dimensionality reduction systems and are explicitly not what I'm trying to model. So I listened to 45,000 audio clips and hacked up a tinder-like swiping system to keep or reject them.\n\nAll the good samples were put through dimensionality reduction. I could have just taken each audio clip and thrown it into the VAE, but each sample contains 176k data points. With only 45k samples, overfitting is likely. To overcome this I reduced the dimensionality of the data using standard signal processing techniques before any ML.\n\n## Dimensionality Reduction\n\nThis was a fun mini-project to see just how much I can compress the sounds without any ML. The goal was to turn a 2 second stereo sound clip (176,400 values) into as few parameters as possible, without losing a significant amount of information.\n\nWhen making the system, my first realization was that if we only want to model timbre, then we don‚Äôt care at all about things changing over time. The only thing that matters is the frequencies of the sound. This kind of thing is perfect for the [Fast Fourier Transform](https://en.wikipedia.org/wiki/Fast_Fourier_transform) (FFT). The FFT essentially extracts the frequencies that make up a sound, and lets you work with the frequencies instead of individual samples in time. \n\n![](assets/latent-fft.png)\n\nAfter applying the FFT, it's easy to throw out lots of information. First, is the phase information. This essentially throws out the data that tells when sounds start or end, which makes up half of the coefficients produced by the FFT. This turns the audio into a monotonous drone, which is fine because the original sound should already be a droning sound. This is equivalent to applying a heavy reverb to the audio. All the same sounds are there, but they're smeared out over time.\n\nNext, we can also throw out stereo audio, because the drone in the right ear is probably the same as the left.\n\nLastly, we can throw out most of the high frequency data. The FFT gives us intensity of each frequency from 0 hz to 22,050 hz, with 0.5 hz increments. This is a good resolution for low frequencies, but human perception of pitch is logarithmic. The difference between 20hz and 20.5hz is audible, but the difference between 20,000hz and 20,000.5hz is inaudible. So, I resampled to combine many of the high frequencies that are indistinguishable. This is called the [CQT](https://en.wikipedia.org/wiki/Constant-Q_transform), it's the same as FFT but with log scaled frequencies.\n\nThen, the values are converted to decibels and fit into the range 0 to 1, since that gives a perceptually linear output, which makes the loss functions align with human hearing.\n\n![](assets/latent-fft2.png)\n\nThis takes the data from 176,400 audio samples all the way down to 1,233 values, a 99.5% reduction. For most audio clips, there is no audible difference in the sound after being compressed in this way.\n\nI later learned this is basically what [MP3](https://en.wikipedia.org/wiki/MP3#Encoding_and_decoding) does to compress audio: use Fourier transforms along with [psychoacoustic](https://en.wikipedia.org/wiki/Psychoacoustics) optimizations. The main difference is that it can handle changing sounds, but the underlying principles are the same.\n\n## VAE\n\n1,233 values is small enough for us to work with, and the reduced dimensionality means that training the model doesn't take long at all, making experimentation quick and easy. Here is the general structure:\n\n![](assets/latent-model.png)\n\n- `UPSAMPLE(X, Y)` means that it is upsampling to X times larger, and padding with Y zeros.\n- All `CONV_1D` layers use same size padding.\n- The `ENCODE_BLOCK` is just a [ResNet block](https://d2l.ai/chapter_convolutional-modern/resnet.html) modified to work with one dimensional data. The `DECODE_BLOCK` is the same, but with inverse (transposed) convolutions. The last of each encode / decode block omits the skip connection.\n\nThe left column is the encoder, it takes our 1233 parameters describing the sound, and compresses it into a 32 dimensional vector. On the right is the decoder, which does the inverse.\n\nSince this is a VAE, the encoder outputs both a mean (Œº) and log-variance (œÉ). When training, these are used to sample a point in a normal distribution with the given mean and log-variance, but at inference, the variance is set to 0 to remove noise. \n\nThe loss function is a combination of [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence), which tries to give the latent space nice properties, like being centered at 0, and preventing values to collapse. The loss function also uses mean squared error to make sure the encoder and decoder are accurately encoding and decoding the data.\n\nTraining is incredibly fast since the input size is already very small. It only takes a few minutes to process all the samples. Once trained, the encoder can be used to map from any sound, to a timbre vector, and back. Inference is also very quick, which is great because a lot of samples are needed during song generation.\n\n# Song Generation\n\nOn its own, the VAE just lets us make a monotonous sound, but not a song. So I added a system that would draw a path in sound space. This could have been done using machine learning, but I found it was unnecessary. A bunch of points are sampled at random, and then are ranked based on some criteria. This criteria varies depending on the desired tone, I change it every time I make an album. I usually rank by pitch, volume, or weirdness (distance from the origin in latent space). From all of the samples, it picks five of the sounds, and repeats them in a structured way. The song is then generated using iterative symbol substitutions\n\nThe song structure starts with the symbol `S` which is then iteratively expanded on using the following rules, selected at random\n\n```\nS => XYXZ\nS => XYZZ\nS => XZYZ\nS => YXYZ\nS => YXZY\nS => YZXZ\nS => YZYZ\nS => ZXYZ\nS => ZXZY\nS => ZYXZ\nX => AABBAABB\nX => ABABAB\nY => CDCD\nY => CCDD\nZ => E\nZ => EE\n```\n\nTo give an example, let's say a song picked these transformations:\n\n```\nS => XZYZ\nX => ABABAB\nY => CCDD\nZ => E\n```\n\nThen applying each rule gives `S => XZYZ => ABABABZYZ => ABABABZCCDDZ => ABABABECCDDE`. The resulting song pattern `ABABABECCDDE` says which of the five sounds to use, and for how long. In this case, we start by going back and forth between the first two sounds (A and B), use point E to transition into a section where C and D alternate slowly, before going back to E at the end. \n\nThis is basically a [context free grammar](https://en.m.wikipedia.org/wiki/Context-free_grammar) but it‚Äôs being used to generate structure instead of parsing it, which gives a lot of flexibility and is very easy to modify.\n\nThis gives us a path through the latent space to make our song. However, the songs generated by this method tend to be pretty boring. Mostly because a 4 minute song with 12 steps has 20 seconds for each step. In other words, the music is just slowly changing sounds every 20 seconds. Although there's a few ambient songs that are [quite literally a single sound for 10 minutes](https://open.spotify.com/track/6y5pnIlIf86X5I6VCgqQhx?si=ae10f3f979a54547), they're the exception, and I'd like a little more variation.\n\nSo, we can spice it up with a tool I call \"the wiggler\". The wiggler takes the straight lines between points in timbre space, and coils it into a helix, giving it constant variation. The wiggling process is simple, it picks two random vectors, multiplies by sin(t) and cos(t) respectively, and adds them to the original path. \n\n> Side note: Technically, this doesn't guarantee a helix. If the random vectors are pointing in the same direction then it will move in a straight line instead of a circle. But a neat thing about high dimensional spaces is that random vectors are almost guaranteed to be at right angles to one another, so this isn't a major concern.\n\nFinally, points on the coiled path are sampled for every 1-2 seconds in the song. Each sample point in timbre space is translated into an audio clip, which is then smoothly blended to produce the final audio. There's a bit of additional processing, like fading in and out, and normalizing the volume, but then we have our audio.\n\n## Effects\n\nSometimes, the songs need a little bit more variation, and that can come from layering some nature sounds on top. This is common practice with ambient music, since it gives some nice variation to the sounds. \n\nI first tried generating these procedurally, and even made a whole framework for making procedural sound effects, but I wasn't happy with the quality in the end. So, instead I pulled some creative commons audio from [freesound](https://freesound.org/). I collected a few dozen nature sounds like wind, rain, thunder, and waves that were good quality and were in the Creative Commons.\n\nI made a simple script to load these sounds in, make them loop seamlessly, adjust their volume to be -30 LUFS, and remove any fading in or out the original clip had.\n\nThen, when a song is generated, it slowly transitions between similar effects. It adds some nice variation and makes it feel less artificial. Once the effects are layered on top of the song, then it's good to go.\n\n# Generating Albums\n\nTo generate a ~15 song album, I tend to generate around 50 songs, and quickly skim through them to see if they're any good. The VAE isn't perfect, and occasionally produces sounds that are too loud or weird, so they have to be removed. This takes about half an hour. After this, I usually have 30 that I find good and interesting. I'll give these a more thorough listening while I'm doing other work, cutting the ones that I don't like, and giving names to the ones I do. I'll also manually clean up the audio as necessary.\n\n# Results\n\n\nSee for yourself! \n\n<audio controls>  \n<source src=\"assets/shoreline.flac\" type=\"audio/flac\">  \nYour browser does not support the audio element.  \n</audio>\n\nThis one is my personal favorite. In my opinion, it's not as good as a lot of the human made stuff, but I'm pretty happy with the results. I've made 4 albums so far, and listen to them from time to time. I was definitely surprised with the variety of sounds you can get out of it, some are more meditative, others are closer to dark ambient music.\n\nMy goal was to make music that someone could listen to, and I think I accomplished that. I had a lot of fun messing with signal processing, and working with VAEs. I think the most interesting part is that the model isn't generating music, it's generating a model of timbre, and that gets used to create the music. That means you spend a lot of time focusing on improving the quality of timbre space, which is an interesting challenge.\n\nOver the years, much more sophisticated music generation models have come out, but I still enjoy how simple and lightweight my solution was."
  },
  {
    "color": "white",
    "width": 3,
    "code": "PRJ.089",
    "title": "Bolt",
    "category": "Project",
    "subtitle": "LLM Assisted Web Development",
    "chip": "bolt",
    "height": 3,
    "order": "013",
    "source": "tonyedv/013 Bolt.md",
    "markdown": "> 2025 note: this was written before coding agents were a thing, so this all seems really obvious now. Most of this tech is pretty standard now (well except the dummy components, which I still think are really neat)\n> \n> Also, Bolt is now the name of an actual product that builds websites. This is 100% a coincidence. I have no relation to that company.\n\nI've been following the development of LLMs since GPT-2, and have been surprised by how useful they are, and after trying [Copilot](https://github.com/features/copilot) I was wondering just how far these systems can go. I figured the technology was so new, that the only way to get a good idea of the capabilities was to try it myself.\n\nUltimately, I built a tool that lets you build websites using plain English and no technical expertise. Simply describe what you want, and Bolt will generate a website. It has full control over the code, allowing for complex, multi-page interactions like shopping carts.\n\n\n![](assets/bolt-cart.gif)\n\nThe above gif shows a site entirely made with Bolt. It took maybe 90 seconds of my input, plus about 10 minutes of waiting for it to make edits. The only framework used was React, all of the interactions like the cart functionality were made entirely by Bolt.\n\n\n## Goal\n\nHave a system utilizing LLMs that can write functional and valuable code. It can get input from a user, as long as it requires no technical skill.\n\n## Non Goals\n\nI don't want to make a general-purpose software developer, to keep things simple, the output will be static web pages.\n\nIt also is not designed to be production-ready, so speed and cost are not prioritized. This is meant as a proof of concept, and not a product.\n\n# Design\n\nThe user says what they want in a textbox on the Bolt interface. Bolt will then use a series of prompts, and an LLM (I'm using GPT-3.5, other LLMs are available) to make the changes.\n\nThere are a couple of parts involved. Each with a single role:\n\n- **API Server** manages everything. It launches the other parts, and has endpoints for asking the LLM to do things and reporting errors.\n- **Site** is the generated website. It uses React, and has additional code that wraps the actual website to give it additional functionality like error detection.\n- **Editor** is the part the user interacts with, it contains an iframe to the site. This is kept separate from the site so that the user can still interact with Bolt when there is a compiler error.\n- **Diode** is a command line utility for checking if a program is valid, it attempts to parse the file with `@babel/parser` and checks for common issues. This is very useful for parsing the LLM outputs.\n\nHere is roughly how they interact\n\n![](assets/bolt-chart.png)\n\n\n# Cascades\n\nCascades are the secret sauce of Bolt. It solves the problem of having a limited context length. The LLMs I'm using can only comprehend 4096 tokens at a time (A token can be a word, a parenthesis, a semicolon, etc.), which means if the website has too much code, it will fail. \n\nVector stores work okay here, but their output is selective and does not give a full description of the code, which is important for more complex edits. I tried summarization, which uses an LLM to summarize the files until they fit in the token limit. This seems like a blunt solution to me, as not all files are equally important. When editing a checkout component, you would want to know exactly how the payment processor is implemented, but the home page is much less important.\n\nTo fix this flaw, I came up with \"cascades\", which is my variant of summarization that dynamically simplifies the data based on how relevant it is to the problem at hand. The less relevant a file is, the more compressed it becomes.\n\nEach file can be simplified to one of several levels. First is `ACTUAL` which is the raw text. Then there's `DOCS` which is the generated documentation for the code. There's also `SUMMARY`, `ONE_LINE`, and `NAME`, each a more simplified version of the file than the last. Most of these use an LLM to generate them, for example, to create a `SUMMARY` version of a file, this prompt is used:\n\n```\nDescribe the code's most important features in plain english. Focus on input props (if any) and their types, as well as imported dependencies. 2 to 5 sentences.\n\n{code}\n```\n\nFiguring out what simplification to use is a classic bin packing problem. We select compression levels for each file to maximize the amount of relevant information, subject to a constraint on the number of tokens we can use. Relevant information is calculated by `relevance * information`.\n\nRelevance is a score from 0 to 1 saying how relevant a file is to the problem at hand. This is done using a combination of LLM feedback, and several heuristics generated by static analysis of the code. For example, if a js file is being edited, then one of the heuristics is to increase the relevance of imported files by +0.5.\n\nInformation is more subjective, I made a heuristic that information is halved by each level of compression, and this seems to give good results.\n\nBin packing is NP-Hard, so we probably don't want a rigorous solution. I went for the greedy approach of adding everything to the bin with no simplification. This will violate the token limit, but we can fix that by iteratively identifying the item with the least relevant information per token, and increasing its compression by one level. This is repeated until the total number of tokens is below some upper bound.\n\n# Error Handling\n\nThe LLM has no idea if the code it generates is valid. It's usually very good, but once there is an error, it cannot be resolved without reading the logs. This violates our goal for the user to not need technical expertise. So, I developed three systems to automatically detect and report errors. \n\nFirst, the API server watches the stderr of the site's server and checks for compiler errors. If it finds one, it'll send the error message to the editor UI and show a fix button. Clicking it will send the details of the error to the LLM with a standard prompt for fixing errors. This works ~80% of the time, and may take a few attempts, but I've never had it get stuck permanently.\n\nSecond, the generated website is wrapped in a React error boundary to catch all runtime errors. If these happen, it will show the same fix error UI. Here is what it looks like when a runtime error is caused by going to the book details page. \n\n![](assets/bolt-error.gif)\n\nLastly, and my personal favorite, before being deployed, the import statements are analyzed to find missing files. If there are any, it creates a \"dummy component\" in place of the missing file, which just contains a textbox asking the user what should go there. Filling it out will tell Bolt to make a new component with the new missing file path.\n\n![](assets/bolt-newcontent.png)\n\nIf all of these systems fail, or if you just don't like the output, there is a revert button that lets you roll back to previous versions of the site.\n\n# CSS\n\nLLMs are terrible with CSS. They're blind and have no way to get feedback on what designs look good. Through my testing, I've learned to give it as little control over the appearance as possible, or else it will accidentally make odd decisions like white text on a white background.\n\nTo fix this, I decided to use a CSS framework, since they can make nice looking websites with minimal CSS. The problem I encountered is that many CSS frameworks expect the developer to override margins and padding to make them presentable.\n\nSo, I did extensive testing of every CSS framework I could find, I must have tried ~15 of them. The key was to get something with good defaults, that was also popular enough that the LLM had been trained on it and knew how to use it. Bulma ended up being the best, and is what it uses now.\n\nAll of the images are pulled from Unsplash, which has a wonderful API to get a random image from a search term: `https://source.unsplash.com/random/\\<width\\>x\\<height\\>/?\\<search\\>`. I told the LLM to use URLs in that format for all images, and it seems to work great. It does pull images at random, so sometimes they aren't the most relevant. It could be improved, but it would require adding more complex systems like CLIP.\n\n# Future Improvements\n\nI'm happy with where the system is at now, it's useful enough that whenever I have a project that needs a simple HTML website, I always use Bolt first to get the broad strokes before cleaning things up myself. But, if I did come back to it, here are some things I'd want to look at.\n\n- API / backend support. Bolt actually has shown that it's capable of making API calls to a backend, but I haven't set up a backend server yet. I could add one and give it a database to let it work on full stack applications.\n- Experiment with 16k context sizes, now that OpenAI added them.\n- Use a more constrained representation of the code, more like Squarespace or WordPress. That would make it look nicer and prevent it from running into as many issues with freeform html / js (EDIT: Looks like [Wix had the same idea](https://www.wix.com/blog/wix-artificial-design-intelligence) and made a system very similar to Bolt in their editor)\n- Improve efficiency: Bolt has to read and regenerate every single file that it modifies, it would be faster and cheaper to just have it output the diffs.\n\nOverall I had a lot of fun with this one, I definitely found a lot of the rough edges with getting LLMs to act agentically and generate code but over time I can see them getting a lot better. Context management is definitely the part that needs the most work at the moment."
  },
  {
    "color": "black",
    "width": 8,
    "height": 2,
    "title": "Get in Touch",
    "subtitle": "",
    "contentInGrid": "true",
    "order": "999",
    "source": "tonyedv/999 Footer.md",
    "markdown": "<span class=\"em\"></span>\n\n[linkedin.com/in/tonyedvalson](https://www.linkedin.com/in/tonyedvalson/)\n\nSite and styes all created by me. No frameworks, no fnord, no templates. Source code is on¬†[GitHub](https://github.com/AnthonyEdvalson/Portfolio)."
  }
];
