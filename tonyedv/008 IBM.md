---
color: cblack
width: 4
code: WRK.IBM
title: Weather Modeling @ IBM
category: Work
subtitle: Using ML to forecast extreme weather
height: 3
chip: ibm
---
Extreme weather causes a lot of damage. In 2022 the US spent [165 billion dollars](https://www.climate.gov/news-features/blogs/2022-us-billion-dollar-weather-and-climate-disasters-historical-context#:~:text=Damages%20from%20the%202022%20disasters%20totaled%20%24165.1%20billion) on damages from disasters. There's a lot of value to be gained by forecasting these events in advance so governments, emergency services, companies, and citizens can better prepare themselves.

While interning at IBM, I had the opportunity to work on an early-stage research project that addresses this issue. IBM acquired [The Weather Company](https://www.ibm.com/weather) back in 2016, and has been using massive amounts of weather data for a [wide](https://www.ibm.com/products/environmental-intelligence-suite/energy-utilities) [variety](https://www.ibm.com/products/environmental-intelligence-suite/weather-safety) [of](https://www.ibm.com/products/environmental-intelligence-suite/environmental-data) [forecasting](https://www.ibm.com/products/environmental-intelligence-suite/agriculture) [tools](https://www.ibm.com/products/environmental-intelligence-suite/risk-management). This project was an experiment to see what other things were possible with this information, as a demonstration of IBM's machine learning capabilities.

We decided to narrow things down to a specific kind of extreme weather event that we could predict easily, and decided to focus on cold snaps. These can be disastrous, particularly in the south where they aren't well equipped to handle the freezing temperatures. This decision was partly influenced by the [winter storm in Texas](https://www.ncei.noaa.gov/news/great-texas-freeze-february-2021#:~:text=On%20February%2011%2D20%2C%202021,the%20entire%20state%20of%20Texas.) that caused the state's power grid to be severely damaged. We figured we could make some progress in predicting these events and were up for the challenge.

# Hasn’t This Been Done Before?

That was the question we started with, because there are plenty of tools that can be used to do long term forecasting already, so we looked into where they were falling short.

We found that all of them only predict averages, as that's what most modern forecasting systems are designed for. This is useful for farmers or government officials who want to know if it will be warmer or cooler this year, but not very useful for finding the risk of disasters, which, by their nature, are outliers.

A lower mean temperature may mean that winter storms are a bit more likely, but variance contributes much more to the risk. We found that modeling this variance or “atmospheric instability” gives a much better idea of when a winter storm is likely. So we figured we could get better results than anyone else if we got really good at modeling the variance.

# Design

![](ibm-flow.png)

We collected data from a few different sources, a mix of IBM's proprietary data, and publicly available data from NOAA. We then did some basic processing before breaking it into the train/dev/test splits.

The model itself is fairly straightforward, PCA is used to reduce dimensionality of the weather data, then the model uses those principal components as inputs.

> Fun fact, the first principal component we identified was almost identical to [El Niño](https://oceanservice.noaa.gov/facts/ninonina.html), which was a nice confirmation that everything was working properly. 

The model consisted of some convolutions, and a few dense layers at the end to get the three output scalars. In between the layers are some batch norms that aren’t shown in the chart. I’d like to include what kind of layers were used, but I’m writing this post ~2 years after the fact and I don’t quite remember the specifics (I believe the convolutions were ResNet blocks, but I could be wrong).

The output `p` is the probability of a freezing temperature over the next two months. We use a standard cross entropy loss on this value.

The other two outputs `μ` and `σ` are the predicted mean and standard deviation of the temperature over the next two months. This is used with a modified categorical cross entropy loss to see how well the actual next two months of data fit the distribution. 

`μ` and `σ` are redundant, since `p` is the data we actually want. But in most places and times of year, the chance of freezing is zero during the summer months, so all our summer data is essentially wasted. By having the model also predict the `μ` and `σ`, the model learns general weather mechanics, even in the months where there is no chance of freezing. This is especially effective in warm areas, where freezing events are very rare.

# Results

The final system was able to predict freezing temperatures with much greater confidence than our baseline. We evaluated it on the southern US and saw that we were able to predict freezing temps significantly better than the baseline. Essentially this meant that in Texas, the old system predicted cold temperatures before they occurred with 13% confidence, but ours does it with 45%. 

This project was a lot of fun, I got to work with some great people and I’m very happy with how the model came out. Before this I had always thought that you would need an obscene amount of computing power or a PhD to make meaningful contributions to the field of Machine learning, but this project made me realize that this wasn’t the case. It’s enough to have a unique perspective and a solid knowledge of the underlying technology.