---
color: white
width: 3
code: PRJ.089
title: Bolt
category: Project
subtitle: LLM Assisted Web Development
chip: bolt
height: 3

---

> 2025 note: this was written before coding agents were a thing, so this all seems really obvious now. Most of this tech is pretty standard now (well except the dummy components, which I still think are really neat)
> 
> Also, Bolt is now the name of an actual product that builds websites. This is 100% a coincidence. I have no relation to that company.

I've been following the development of LLMs since GPT-2, and have been surprised by how useful they are, and after trying [Copilot](https://github.com/features/copilot) I was wondering just how far these systems can go. I figured the technology was so new, that the only way to get a good idea of the capabilities was to try it myself.

Ultimately, I built a tool that lets you build websites using plain English and no technical expertise. Simply describe what you want, and Bolt will generate a website. It has full control over the code, allowing for complex, multi-page interactions like shopping carts.


![](assets/bolt-cart.gif)

The above gif shows a site entirely made with Bolt. It took maybe 90 seconds of my input, plus about 10 minutes of waiting for it to make edits. The only framework used was React, all of the interactions like the cart functionality were made entirely by Bolt.


## Goal

Have a system utilizing LLMs that can write functional and valuable code. It can get input from a user, as long as it requires no technical skill.

## Non Goals

I don't want to make a general-purpose software developer, to keep things simple, the output will be static web pages.

It also is not designed to be production-ready, so speed and cost are not prioritized. This is meant as a proof of concept, and not a product.

# Design

The user says what they want in a textbox on the Bolt interface. Bolt will then use a series of prompts, and an LLM (I'm using GPT-3.5, other LLMs are available) to make the changes.

There are a couple of parts involved. Each with a single role:

- **API Server** manages everything. It launches the other parts, and has endpoints for asking the LLM to do things and reporting errors.
- **Site** is the generated website. It uses React, and has additional code that wraps the actual website to give it additional functionality like error detection.
- **Editor** is the part the user interacts with, it contains an iframe to the site. This is kept separate from the site so that the user can still interact with Bolt when there is a compiler error.
- **Diode** is a command line utility for checking if a program is valid, it attempts to parse the file with `@babel/parser` and checks for common issues. This is very useful for parsing the LLM outputs.

Here is roughly how they interact

![](assets/bolt-chart.png)


# Cascades

Cascades are the secret sauce of Bolt. It solves the problem of having a limited context length. The LLMs I'm using can only comprehend 4096 tokens at a time (A token can be a word, a parenthesis, a semicolon, etc.), which means if the website has too much code, it will fail. 

Vector stores work okay here, but their output is selective and does not give a full description of the code, which is important for more complex edits. I tried summarization, which uses an LLM to summarize the files until they fit in the token limit. This seems like a blunt solution to me, as not all files are equally important. When editing a checkout component, you would want to know exactly how the payment processor is implemented, but the home page is much less important.

To fix this flaw, I came up with "cascades", which is my variant of summarization that dynamically simplifies the data based on how relevant it is to the problem at hand. The less relevant a file is, the more compressed it becomes.

Each file can be simplified to one of several levels. First is `ACTUAL` which is the raw text. Then there's `DOCS` which is the generated documentation for the code. There's also `SUMMARY`, `ONE_LINE`, and `NAME`, each a more simplified version of the file than the last. Most of these use an LLM to generate them, for example, to create a `SUMMARY` version of a file, this prompt is used:

```
Describe the code's most important features in plain english. Focus on input props (if any) and their types, as well as imported dependencies. 2 to 5 sentences.

{code}
```

Figuring out what simplification to use is a classic bin packing problem. We select compression levels for each file to maximize the amount of relevant information, subject to a constraint on the number of tokens we can use. Relevant information is calculated by `relevance * information`.

Relevance is a score from 0 to 1 saying how relevant a file is to the problem at hand. This is done using a combination of LLM feedback, and several heuristics generated by static analysis of the code. For example, if a js file is being edited, then one of the heuristics is to increase the relevance of imported files by +0.5.

Information is more subjective, I made a heuristic that information is halved by each level of compression, and this seems to give good results.

Bin packing is NP-Hard, so we probably don't want a rigorous solution. I went for the greedy approach of adding everything to the bin with no simplification. This will violate the token limit, but we can fix that by iteratively identifying the item with the least relevant information per token, and increasing its compression by one level. This is repeated until the total number of tokens is below some upper bound.

# Error Handling

The LLM has no idea if the code it generates is valid. It's usually very good, but once there is an error, it cannot be resolved without reading the logs. This violates our goal for the user to not need technical expertise. So, I developed three systems to automatically detect and report errors. 

First, the API server watches the stderr of the site's server and checks for compiler errors. If it finds one, it'll send the error message to the editor UI and show a fix button. Clicking it will send the details of the error to the LLM with a standard prompt for fixing errors. This works ~80% of the time, and may take a few attempts, but I've never had it get stuck permanently.

Second, the generated website is wrapped in a React error boundary to catch all runtime errors. If these happen, it will show the same fix error UI. Here is what it looks like when a runtime error is caused by going to the book details page. 

![](assets/bolt-error.gif)

Lastly, and my personal favorite, before being deployed, the import statements are analyzed to find missing files. If there are any, it creates a "dummy component" in place of the missing file, which just contains a textbox asking the user what should go there. Filling it out will tell Bolt to make a new component with the new missing file path.

![](assets/bolt-newcontent.png)

If all of these systems fail, or if you just don't like the output, there is a revert button that lets you roll back to previous versions of the site.

# CSS

LLMs are terrible with CSS. They're blind and have no way to get feedback on what designs look good. Through my testing, I've learned to give it as little control over the appearance as possible, or else it will accidentally make odd decisions like white text on a white background.

To fix this, I decided to use a CSS framework, since they can make nice looking websites with minimal CSS. The problem I encountered is that many CSS frameworks expect the developer to override margins and padding to make them presentable.

So, I did extensive testing of every CSS framework I could find, I must have tried ~15 of them. The key was to get something with good defaults, that was also popular enough that the LLM had been trained on it and knew how to use it. Bulma ended up being the best, and is what it uses now.

All of the images are pulled from Unsplash, which has a wonderful API to get a random image from a search term: `https://source.unsplash.com/random/\<width\>x\<height\>/?\<search\>`. I told the LLM to use URLs in that format for all images, and it seems to work great. It does pull images at random, so sometimes they aren't the most relevant. It could be improved, but it would require adding more complex systems like CLIP.

# Future Improvements

I'm happy with where the system is at now, it's useful enough that whenever I have a project that needs a simple HTML website, I always use Bolt first to get the broad strokes before cleaning things up myself. But, if I did come back to it, here are some things I'd want to look at.

- API / backend support. Bolt actually has shown that it's capable of making API calls to a backend, but I haven't set up a backend server yet. I could add one and give it a database to let it work on full stack applications.
- Experiment with 16k context sizes, now that OpenAI added them.
- Use a more constrained representation of the code, more like Squarespace or WordPress. That would make it look nicer and prevent it from running into as many issues with freeform html / js (EDIT: Looks like [Wix had the same idea](https://www.wix.com/blog/wix-artificial-design-intelligence) and made a system very similar to Bolt in their editor)
- Improve efficiency: Bolt has to read and regenerate every single file that it modifies, it would be faster and cheaper to just have it output the diffs.

Overall I had a lot of fun with this one, I definitely found a lot of the rough edges with getting LLMs to act agentically and generate code but over time I can see them getting a lot better. Context management is definitely the part that needs the most work at the moment.